{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skin-Types-Ext-RAM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milanpavikevik/Skin-Lesion-Classification/blob/main/Skin_Types-First-Try-Default.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL2tzXOzHS9b",
        "outputId": "78365329-7f1e-49bd-eab1-56e111643577"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhTMJR9ZHTGF",
        "outputId": "ec5614af-41d4-4b5e-f4d5-7ce072cc0361"
      },
      "source": [
        "import warnings                        # To ignore any warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "%matplotlib inline\r\n",
        "%pylab inline\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import itertools\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# Audio\r\n",
        "import librosa\r\n",
        "import librosa.display\r\n",
        "\r\n",
        "# Scikit learn\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.utils import class_weight\r\n",
        "\r\n",
        "# Keras\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D, MaxPool2D\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wEiZ7VZQiVD"
      },
      "source": [
        "testiraj golemina na slika 768x768 , 512x512 , 384x384 , 256x256 , 192x192"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUMaUf0lHTJL",
        "outputId": "997b9685-e584-45c9-d3de-993fb976fad4"
      },
      "source": [
        "INPUT_DIR = '/content/drive/MyDrive/Skin-Lesion/**'\r\n",
        " \r\n",
        "dataset = []\r\n",
        "for filename in glob.iglob(INPUT_DIR):\r\n",
        "    kolkuPoKlasa=0\r\n",
        "    print(filename)\r\n",
        "    for f in glob.iglob(filename+'/**'):\r\n",
        "      #print(f)\r\n",
        "      kolkuPoKlasa+=1\r\n",
        "      if (kolkuPoKlasa > 1000):\r\n",
        "        kolkuPoKlasa=0\r\n",
        "        break\r\n",
        "        \r\n",
        "    \r\n",
        "      \r\n",
        "      image= cv2.imread( f, cv2.COLOR_BGR2RGB)\r\n",
        "      image=cv2.resize(image, (256, 256),interpolation = cv2.INTER_AREA)\r\n",
        "      image=np.array(image)\r\n",
        "      image = image.astype('float32')\r\n",
        "      image /= 255 \r\n",
        "      \r\n",
        "      class_name = filename.split('/')[-1]\r\n",
        "      \r\n",
        "      dataset.append([image,class_name])\r\n",
        "      \r\n",
        "      \r\n",
        "    \r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Skin-Lesion/BCC\n",
            "/content/drive/MyDrive/Skin-Lesion/DF\n",
            "/content/drive/MyDrive/Skin-Lesion/AK\n",
            "/content/drive/MyDrive/Skin-Lesion/BKL\n",
            "/content/drive/MyDrive/Skin-Lesion/NV\n",
            "/content/drive/MyDrive/Skin-Lesion/VASC\n",
            "/content/drive/MyDrive/Skin-Lesion/MEL\n",
            "/content/drive/MyDrive/Skin-Lesion/SCC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YStzGE87HTMy",
        "outputId": "8bc1204b-3a5c-48d2-e306-1c7097a9ea9e"
      },
      "source": [
        "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\r\n",
        "x_train = []\r\n",
        "y_train = []\r\n",
        "for img,klasa in train:\r\n",
        "  x_train.append(img)\r\n",
        "  y_train.append(klasa)\r\n",
        "\r\n",
        "x_test = []\r\n",
        "y_test = []\r\n",
        "for img,klasa in test[:int(len(test)*0.5)]:\r\n",
        "  x_test.append(img)\r\n",
        "  y_test.append(klasa)\r\n",
        "\r\n",
        "\r\n",
        "x_val = []\r\n",
        "y_val = []\r\n",
        "for img,klasa in test[int(len(test)*0.5):]:\r\n",
        "  x_val.append(img)\r\n",
        "  y_val.append(klasa)\r\n",
        "\r\n",
        "print(len(x_train),len(y_train))\r\n",
        "print(len(y_test),len(y_test))\r\n",
        "print(len(x_val),len(y_val))\r\n",
        "\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(y_train)\r\n",
        "\r\n",
        "y_train = encoder.transform(y_train)\r\n",
        "\r\n",
        "y_test =  encoder.transform(y_test)\r\n",
        "y_val =   encoder.transform(y_val)\r\n",
        "\r\n",
        "\r\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\r\n",
        "print(class_weight)\r\n",
        "\r\n",
        "x_test = np.asarray(x_test)\r\n",
        "x_val = np.asarray(x_val)\r\n",
        "x_train = np.asarray(x_train)\r\n",
        "\r\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)\r\n",
        "x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], x_val.shape[2], 3)\r\n",
        "\r\n",
        "y_train = to_categorical(y_train)\r\n",
        "y_test = to_categorical(y_test)\r\n",
        "y_val = to_categorical(y_val)\r\n",
        "\r\n",
        "print(\"X train:\", x_train.shape)\r\n",
        "print(\"Y train:\", y_train.shape)\r\n",
        "print(\"X test:\", x_test.shape)\r\n",
        "print(\"Y test:\", y_test.shape)\r\n",
        "\r\n",
        "print(\"X validation:\", x_val.shape)\r\n",
        "print(\"Y validation:\", y_val.shape)\r\n",
        "\r\n",
        "print(class_weights)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4789 4789\n",
            "599 599\n",
            "599 599\n",
            "<module 'sklearn.utils.class_weight' from '/usr/local/lib/python3.7/dist-packages/sklearn/utils/class_weight.py'>\n",
            "X train: (4789, 256, 256, 3)\n",
            "Y train: (4789, 8)\n",
            "X test: (599, 256, 256, 3)\n",
            "Y test: (599, 8)\n",
            "X validation: (599, 256, 256, 3)\n",
            "Y validation: (599, 8)\n",
            "[0.84791076 0.7596764  0.74921777 3.40127841 0.73631611 0.74363354\n",
            " 1.20934343 2.89190821]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "_SmbIOPmHTPC",
        "outputId": "22fd655e-9b67-4096-c93e-71159a0d6c72"
      },
      "source": [
        "import matplotlib\r\n",
        "matplotlib.rcParams['figure.figsize'] = (14, 5)\r\n",
        "matplotlib.rcParams['font.size'] = 12\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "num_classes = 8\r\n",
        "\r\n",
        "xlass=[]\r\n",
        "\r\n",
        "for row in dataset:\r\n",
        "  xlass.append(row[1])\r\n",
        "print(len(xlass))\r\n",
        "xlass = pd.DataFrame(xlass)\r\n",
        "\r\n",
        "class_counts = xlass.value_counts()\r\n",
        "cmap = plt.cm.get_cmap(plt.cm.Set3, 10)\r\n",
        "colors = [cmap(i) for i in range(num_classes)]\r\n",
        "\r\n",
        "plt.barh(range(num_classes)[::-1], class_counts, tick_label=xlass[0].unique(),\r\n",
        "         color=colors)\r\n",
        "plt.title('Class distribution of dataset')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "xlass=[]\r\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0Rm11BCtW_Q",
        "outputId": "de70c843-cb99-4b94-dabe-9e29403d33b4"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]),\r\n",
        "                 activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=2))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "\r\n",
        "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=2))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "\r\n",
        "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=2))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "\r\n",
        "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=2))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(GlobalAveragePooling2D())\r\n",
        "\r\n",
        "model.add(Dense(len(encoder.classes_), activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001)\r\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 255, 255, 16)      208       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 127, 127, 16)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 127, 127, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 126, 126, 32)      2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 62, 62, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 31, 31, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 31, 31, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 30, 30, 128)       32896     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 15, 15, 128)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8)                 1032      \n",
            "=================================================================\n",
            "Total params: 44,472\n",
            "Trainable params: 44,472\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4meu5npDtXK3",
        "outputId": "b5f12cb7-13ad-4c3f-fa79-8ef26de6f6da"
      },
      "source": [
        "history = model.fit(x_train, y_train,\r\n",
        "                    batch_size=128,\r\n",
        "                    epochs=100,\r\n",
        "                    validation_data=(x_val, y_val))\r\n",
        "\r\n",
        "\r\n",
        "model_name = \"/content/skin-classical.h5\"\r\n",
        "model.save(model_name)\r\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "38/38 [==============================] - 129s 3s/step - loss: 2.0133 - accuracy: 0.1569 - val_loss: 2.0364 - val_accuracy: 0.1619\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.9704 - accuracy: 0.1874 - val_loss: 1.9619 - val_accuracy: 0.2220\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.8637 - accuracy: 0.2299 - val_loss: 1.8841 - val_accuracy: 0.2621\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.8245 - accuracy: 0.2703 - val_loss: 1.8728 - val_accuracy: 0.2521\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.8100 - accuracy: 0.2763 - val_loss: 1.8622 - val_accuracy: 0.2454\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.7691 - accuracy: 0.2783 - val_loss: 1.8552 - val_accuracy: 0.2688\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 129s 3s/step - loss: 1.7653 - accuracy: 0.2855 - val_loss: 1.8260 - val_accuracy: 0.2487\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.7650 - accuracy: 0.2837 - val_loss: 1.8198 - val_accuracy: 0.2571\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.7303 - accuracy: 0.3121 - val_loss: 1.8170 - val_accuracy: 0.2688\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 129s 3s/step - loss: 1.7346 - accuracy: 0.2911 - val_loss: 1.8096 - val_accuracy: 0.2821\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.7269 - accuracy: 0.3112 - val_loss: 1.8133 - val_accuracy: 0.2421\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.7076 - accuracy: 0.3166 - val_loss: 1.8142 - val_accuracy: 0.2621\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 130s 3s/step - loss: 1.7257 - accuracy: 0.2997 - val_loss: 1.8145 - val_accuracy: 0.3055\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.7071 - accuracy: 0.3250 - val_loss: 1.7981 - val_accuracy: 0.3122\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.7228 - accuracy: 0.3245 - val_loss: 1.7920 - val_accuracy: 0.3005\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6995 - accuracy: 0.3272 - val_loss: 1.7844 - val_accuracy: 0.2938\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6923 - accuracy: 0.3189 - val_loss: 1.7689 - val_accuracy: 0.3222\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.7017 - accuracy: 0.3299 - val_loss: 1.7831 - val_accuracy: 0.2955\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.6908 - accuracy: 0.3358 - val_loss: 1.7611 - val_accuracy: 0.3172\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6824 - accuracy: 0.3353 - val_loss: 1.7657 - val_accuracy: 0.3272\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6817 - accuracy: 0.3401 - val_loss: 1.7545 - val_accuracy: 0.3022\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6801 - accuracy: 0.3421 - val_loss: 1.7464 - val_accuracy: 0.3339\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.6626 - accuracy: 0.3450 - val_loss: 1.7408 - val_accuracy: 0.3439\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6555 - accuracy: 0.3514 - val_loss: 1.7391 - val_accuracy: 0.3222\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6729 - accuracy: 0.3450 - val_loss: 1.7327 - val_accuracy: 0.3339\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6540 - accuracy: 0.3496 - val_loss: 1.7218 - val_accuracy: 0.3456\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6604 - accuracy: 0.3436 - val_loss: 1.7257 - val_accuracy: 0.3456\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6775 - accuracy: 0.3479 - val_loss: 1.7321 - val_accuracy: 0.3406\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6324 - accuracy: 0.3605 - val_loss: 1.7221 - val_accuracy: 0.3673\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6338 - accuracy: 0.3497 - val_loss: 1.7146 - val_accuracy: 0.3539\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6211 - accuracy: 0.3599 - val_loss: 1.7248 - val_accuracy: 0.3222\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.6339 - accuracy: 0.3646 - val_loss: 1.7364 - val_accuracy: 0.3272\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6343 - accuracy: 0.3640 - val_loss: 1.7094 - val_accuracy: 0.3456\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6091 - accuracy: 0.3709 - val_loss: 1.7059 - val_accuracy: 0.3523\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6477 - accuracy: 0.3483 - val_loss: 1.7354 - val_accuracy: 0.3306\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6245 - accuracy: 0.3624 - val_loss: 1.6996 - val_accuracy: 0.3623\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6189 - accuracy: 0.3607 - val_loss: 1.7248 - val_accuracy: 0.3406\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6285 - accuracy: 0.3541 - val_loss: 1.7203 - val_accuracy: 0.3539\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6366 - accuracy: 0.3669 - val_loss: 1.7247 - val_accuracy: 0.3322\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 129s 3s/step - loss: 1.6198 - accuracy: 0.3633 - val_loss: 1.7185 - val_accuracy: 0.3339\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 125s 3s/step - loss: 1.6288 - accuracy: 0.3644 - val_loss: 1.7069 - val_accuracy: 0.3456\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6362 - accuracy: 0.3568 - val_loss: 1.7033 - val_accuracy: 0.3489\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6085 - accuracy: 0.3668 - val_loss: 1.7108 - val_accuracy: 0.3472\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6113 - accuracy: 0.3604 - val_loss: 1.6935 - val_accuracy: 0.3523\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6283 - accuracy: 0.3626 - val_loss: 1.6986 - val_accuracy: 0.3422\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6017 - accuracy: 0.3611 - val_loss: 1.7103 - val_accuracy: 0.3439\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6099 - accuracy: 0.3693 - val_loss: 1.7049 - val_accuracy: 0.3372\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6444 - accuracy: 0.3374 - val_loss: 1.6933 - val_accuracy: 0.3539\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6154 - accuracy: 0.3639 - val_loss: 1.6977 - val_accuracy: 0.3656\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6017 - accuracy: 0.3721 - val_loss: 1.6857 - val_accuracy: 0.3539\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6005 - accuracy: 0.3705 - val_loss: 1.6910 - val_accuracy: 0.3556\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6055 - accuracy: 0.3673 - val_loss: 1.7015 - val_accuracy: 0.3439\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5954 - accuracy: 0.3727 - val_loss: 1.7045 - val_accuracy: 0.3406\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5975 - accuracy: 0.3760 - val_loss: 1.6794 - val_accuracy: 0.3673\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5987 - accuracy: 0.3841 - val_loss: 1.6938 - val_accuracy: 0.3673\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5960 - accuracy: 0.3827 - val_loss: 1.6870 - val_accuracy: 0.3639\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5987 - accuracy: 0.3604 - val_loss: 1.6904 - val_accuracy: 0.3606\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6008 - accuracy: 0.3737 - val_loss: 1.6795 - val_accuracy: 0.3589\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5966 - accuracy: 0.3741 - val_loss: 1.6901 - val_accuracy: 0.3589\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.6047 - accuracy: 0.3680 - val_loss: 1.6893 - val_accuracy: 0.3456\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5946 - accuracy: 0.3773 - val_loss: 1.6872 - val_accuracy: 0.3523\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5889 - accuracy: 0.3821 - val_loss: 1.6946 - val_accuracy: 0.3456\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6012 - accuracy: 0.3775 - val_loss: 1.6958 - val_accuracy: 0.3389\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6151 - accuracy: 0.3717 - val_loss: 1.6976 - val_accuracy: 0.3556\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5829 - accuracy: 0.3692 - val_loss: 1.6858 - val_accuracy: 0.3539\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6179 - accuracy: 0.3683 - val_loss: 1.7294 - val_accuracy: 0.3489\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6341 - accuracy: 0.3570 - val_loss: 1.6961 - val_accuracy: 0.3523\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5983 - accuracy: 0.3689 - val_loss: 1.6940 - val_accuracy: 0.3489\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6076 - accuracy: 0.3638 - val_loss: 1.6848 - val_accuracy: 0.3790\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5886 - accuracy: 0.3813 - val_loss: 1.6670 - val_accuracy: 0.3623\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5825 - accuracy: 0.3825 - val_loss: 1.6885 - val_accuracy: 0.3489\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5877 - accuracy: 0.3780 - val_loss: 1.6695 - val_accuracy: 0.3756\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5724 - accuracy: 0.3813 - val_loss: 1.6829 - val_accuracy: 0.3506\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5789 - accuracy: 0.3893 - val_loss: 1.6720 - val_accuracy: 0.3489\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5899 - accuracy: 0.3796 - val_loss: 1.7068 - val_accuracy: 0.3523\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5628 - accuracy: 0.3948 - val_loss: 1.6785 - val_accuracy: 0.3623\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5704 - accuracy: 0.3811 - val_loss: 1.6933 - val_accuracy: 0.3656\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5828 - accuracy: 0.3815 - val_loss: 1.6655 - val_accuracy: 0.3756\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5772 - accuracy: 0.3736 - val_loss: 1.6771 - val_accuracy: 0.3639\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5854 - accuracy: 0.3828 - val_loss: 1.6664 - val_accuracy: 0.3856\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5728 - accuracy: 0.3905 - val_loss: 1.6724 - val_accuracy: 0.3706\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5821 - accuracy: 0.3792 - val_loss: 1.6830 - val_accuracy: 0.3456\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5878 - accuracy: 0.3829 - val_loss: 1.6643 - val_accuracy: 0.3589\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5708 - accuracy: 0.3737 - val_loss: 1.6930 - val_accuracy: 0.3489\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5690 - accuracy: 0.3944 - val_loss: 1.6654 - val_accuracy: 0.3639\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 125s 3s/step - loss: 1.5988 - accuracy: 0.3723 - val_loss: 1.6725 - val_accuracy: 0.3523\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5919 - accuracy: 0.3763 - val_loss: 1.6921 - val_accuracy: 0.3556\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5748 - accuracy: 0.3814 - val_loss: 1.6780 - val_accuracy: 0.3556\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5683 - accuracy: 0.3784 - val_loss: 1.7292 - val_accuracy: 0.3689\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5847 - accuracy: 0.3712 - val_loss: 1.6672 - val_accuracy: 0.3589\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5774 - accuracy: 0.3798 - val_loss: 1.6810 - val_accuracy: 0.3472\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5896 - accuracy: 0.3782 - val_loss: 1.6676 - val_accuracy: 0.3806\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5459 - accuracy: 0.4021 - val_loss: 1.6764 - val_accuracy: 0.3656\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.5596 - accuracy: 0.3892 - val_loss: 1.6599 - val_accuracy: 0.3673\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5564 - accuracy: 0.3861 - val_loss: 1.6711 - val_accuracy: 0.3656\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 125s 3s/step - loss: 1.5779 - accuracy: 0.3881 - val_loss: 1.6605 - val_accuracy: 0.3573\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6095 - accuracy: 0.3750 - val_loss: 1.6603 - val_accuracy: 0.3689\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5567 - accuracy: 0.3929 - val_loss: 1.6710 - val_accuracy: 0.3573\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5730 - accuracy: 0.4001 - val_loss: 1.6630 - val_accuracy: 0.3606\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.5627 - accuracy: 0.3907 - val_loss: 1.6703 - val_accuracy: 0.3639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDFPvny8t_oG",
        "outputId": "87523452-c202-4192-e555-fc33ece436b6"
      },
      "source": [
        "predictions = model.predict(x_test, verbose=1)\r\n",
        "\r\n",
        "y_true, y_pred = [],[]\r\n",
        "classes = encoder.classes_\r\n",
        "for idx, prediction in enumerate(predictions): \r\n",
        "    y_true.append(classes[np.argmax(y_test[idx])])\r\n",
        "    y_pred.append(classes[np.argmax(prediction)])\r\n",
        "    \r\n",
        "print(classification_report(y_pred, y_true))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 5s 247ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AK       0.29      0.22      0.25       104\n",
            "         BCC       0.50      0.28      0.36       203\n",
            "         BKL       0.13      0.48      0.21        29\n",
            "          DF       0.00      0.00      0.00         0\n",
            "         MEL       0.37      0.44      0.40        75\n",
            "          NV       0.60      0.39      0.48       152\n",
            "         SCC       0.15      0.39      0.22        23\n",
            "        VASC       0.41      0.69      0.51        13\n",
            "\n",
            "    accuracy                           0.34       599\n",
            "   macro avg       0.31      0.36      0.30       599\n",
            "weighted avg       0.44      0.34      0.37       599\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWEjDP6_5UHu"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "# convolutional layer\r\n",
        "model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3])))\r\n",
        "\r\n",
        "# convolutional layer\r\n",
        "model.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "\r\n",
        "model.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "\r\n",
        "# flatten output of conv\r\n",
        "model.add(Flatten())\r\n",
        "\r\n",
        "# hidden layer\r\n",
        "model.add(Dense(500, activation='relu'))\r\n",
        "model.add(Dropout(0.4))\r\n",
        "model.add(Dense(250, activation='relu'))\r\n",
        "model.add(Dropout(0.3))\r\n",
        "# output layer\r\n",
        "model.add(Dense(8, activation='softmax'))\r\n",
        "\r\n",
        "# compiling the sequential model\r\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\r\n",
        "\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsCtEALu5URe",
        "outputId": "5f5b1c08-6e41-4eda-9ede-89d6fcea431a"
      },
      "source": [
        "history1 = model.fit(x_train, y_train,\r\n",
        "                    batch_size=128,\r\n",
        "                    epochs=10,\r\n",
        "                    validation_data=(x_val, y_val))\r\n",
        "\r\n",
        "\r\n",
        "model_name = \"/content/skin-type-1.h5\"\r\n",
        "model.save(model_name)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 1976s 52s/step - loss: 6.9064 - accuracy: 0.1548 - val_loss: 2.0622 - val_accuracy: 0.1436\n",
            "38/38 [==============================] - 1976s 52s/step - loss: 6.9064 - accuracy: 0.1548 - val_loss: 2.0622 - val_accuracy: 0.1436\n",
            "Epoch 2/10\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 1896s 50s/step - loss: 1.9537 - accuracy: 0.1754 - val_loss: 2.0376 - val_accuracy: 0.1486\n",
            "38/38 [==============================] - 1896s 50s/step - loss: 1.9537 - accuracy: 0.1754 - val_loss: 2.0376 - val_accuracy: 0.1486\n",
            "Epoch 3/10\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 1911s 50s/step - loss: 1.9217 - accuracy: 0.2030 - val_loss: 2.0407 - val_accuracy: 0.1386\n",
            "38/38 [==============================] - 1911s 50s/step - loss: 1.9217 - accuracy: 0.2030 - val_loss: 2.0407 - val_accuracy: 0.1386\n",
            "Epoch 4/10\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 1891s 50s/step - loss: 1.9180 - accuracy: 0.2287 - val_loss: 2.0201 - val_accuracy: 0.1369\n",
            "38/38 [==============================] - 1891s 50s/step - loss: 1.9180 - accuracy: 0.2287 - val_loss: 2.0201 - val_accuracy: 0.1369\n",
            "Epoch 5/10\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 1881s 50s/step - loss: 1.8863 - accuracy: 0.2482 - val_loss: 1.9558 - val_accuracy: 0.2137\n",
            "38/38 [==============================] - 1881s 50s/step - loss: 1.8863 - accuracy: 0.2482 - val_loss: 1.9558 - val_accuracy: 0.2137\n",
            "Epoch 6/10\n",
            "Epoch 6/10\n",
            "38/38 [==============================] - 1893s 50s/step - loss: 1.8627 - accuracy: 0.2596 - val_loss: 1.9403 - val_accuracy: 0.1836\n",
            "38/38 [==============================] - 1893s 50s/step - loss: 1.8627 - accuracy: 0.2596 - val_loss: 1.9403 - val_accuracy: 0.1836\n",
            "Epoch 7/10\n",
            "Epoch 7/10\n",
            "38/38 [==============================] - 1874s 49s/step - loss: 1.8316 - accuracy: 0.2731 - val_loss: 1.9476 - val_accuracy: 0.1970\n",
            "38/38 [==============================] - 1874s 49s/step - loss: 1.8316 - accuracy: 0.2731 - val_loss: 1.9476 - val_accuracy: 0.1970\n",
            "Epoch 8/10\n",
            "Epoch 8/10\n",
            "38/38 [==============================] - 1876s 49s/step - loss: 1.8099 - accuracy: 0.2673 - val_loss: 1.8651 - val_accuracy: 0.2571\n",
            "38/38 [==============================] - 1876s 49s/step - loss: 1.8099 - accuracy: 0.2673 - val_loss: 1.8651 - val_accuracy: 0.2571\n",
            "Epoch 9/10\n",
            "Epoch 9/10\n",
            "38/38 [==============================] - 1849s 49s/step - loss: 1.7562 - accuracy: 0.3059 - val_loss: 1.8237 - val_accuracy: 0.2671\n",
            "38/38 [==============================] - 1849s 49s/step - loss: 1.7562 - accuracy: 0.3059 - val_loss: 1.8237 - val_accuracy: 0.2671\n",
            "Epoch 10/10\n",
            "Epoch 10/10\n",
            "38/38 [==============================] - 1921s 51s/step - loss: 1.7090 - accuracy: 0.3240 - val_loss: 1.7423 - val_accuracy: 0.3005\n",
            "38/38 [==============================] - 1921s 51s/step - loss: 1.7090 - accuracy: 0.3240 - val_loss: 1.7423 - val_accuracy: 0.3005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lda0p1H-qRCl",
        "outputId": "00e96c76-5df5-4e88-eaab-8b015410011b"
      },
      "source": [
        "predictions = model.predict(x_test, verbose=1)\r\n",
        "\r\n",
        "y_true, y_pred = [],[]\r\n",
        "classes = encoder.classes_\r\n",
        "for idx, prediction in enumerate(predictions): \r\n",
        "    y_true.append(classes[np.argmax(y_test[idx])])\r\n",
        "    y_pred.append(classes[np.argmax(prediction)])\r\n",
        "    \r\n",
        "print(classification_report(y_pred, y_true))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 54s 3s/step\n",
            "19/19 [==============================] - 54s 3s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AK       0.37      0.30      0.33        96\n",
            "         BCC       0.52      0.34      0.41       171\n",
            "         BKL       0.22      0.25      0.23        91\n",
            "          DF       0.00      0.00      0.00         0\n",
            "         MEL       0.18      0.57      0.27        28\n",
            "          NV       0.49      0.40      0.44       124\n",
            "         SCC       0.36      0.28      0.32        74\n",
            "        VASC       0.32      0.47      0.38        15\n",
            "\n",
            "    accuracy                           0.34       599\n",
            "   macro avg       0.31      0.33      0.30       599\n",
            "weighted avg       0.40      0.34      0.36       599\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AK       0.37      0.30      0.33        96\n",
            "         BCC       0.52      0.34      0.41       171\n",
            "         BKL       0.22      0.25      0.23        91\n",
            "          DF       0.00      0.00      0.00         0\n",
            "         MEL       0.18      0.57      0.27        28\n",
            "          NV       0.49      0.40      0.44       124\n",
            "         SCC       0.36      0.28      0.32        74\n",
            "        VASC       0.32      0.47      0.38        15\n",
            "\n",
            "    accuracy                           0.34       599\n",
            "   macro avg       0.31      0.33      0.30       599\n",
            "weighted avg       0.40      0.34      0.36       599\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxh5qfQv6LIh"
      },
      "source": [
        "hyper-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPb3EoGp5UUE",
        "outputId": "05bee4d8-18e0-4f1f-9a94-988f2c8f1eb6"
      },
      "source": [
        "from keras.applications import VGG16\r\n",
        "\r\n",
        "# include top should be False to remove the softmax layer\r\n",
        "pretrained_model = VGG16(include_top=False, weights='imagenet')\r\n",
        "pretrained_model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "byX90SPN5UXo",
        "outputId": "f60f6c20-d3e6-4cd1-fe3c-79877c23341c"
      },
      "source": [
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "vgg_features_train = pretrained_model.predict(x_train)\r\n",
        "vgg_features_val = pretrained_model.predict(x_val)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b851c8875d9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvgg_features_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvgg_features_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:207 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer vgg16 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 256, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 8) dtype=float32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkIWRQtp6Tuf"
      },
      "source": [
        "train_target = y_train\r\n",
        "val_target = y_val"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Buz-AG6TyG",
        "outputId": "506f5056-22ec-42e6-b23a-a3875d54cfe6"
      },
      "source": [
        "from keras.layers.normalization import BatchNormalization\r\n",
        "model2 = Sequential()\r\n",
        "model2.add(Flatten(input_shape=(8,8,512)))\r\n",
        "model2.add(Dense(100, activation='relu'))\r\n",
        "model2.add(Dropout(0.5))\r\n",
        "model2.add(BatchNormalization())\r\n",
        "model2.add(Dense(8, activation='softmax'))\r\n",
        "\r\n",
        "# compile the model\r\n",
        "model2.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\r\n",
        "\r\n",
        "model2.summary()\r\n",
        "\r\n",
        "# train model using features generated from VGG16 model\r\n",
        "model2.fit(vgg_features_train, train_target, epochs=100, batch_size=128, validation_data=(vgg_features_val, val_target))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_5 (Flatten)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               3276900   \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 8)                 808       \n",
            "=================================================================\n",
            "Total params: 3,278,108\n",
            "Trainable params: 3,277,908\n",
            "Non-trainable params: 200\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 4s 81ms/step - loss: 2.2800 - accuracy: 0.1680 - val_loss: 1.9480 - val_accuracy: 0.2154\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.9286 - accuracy: 0.2643 - val_loss: 1.8650 - val_accuracy: 0.2571\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.7870 - accuracy: 0.3164 - val_loss: 1.7845 - val_accuracy: 0.3122\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.6726 - accuracy: 0.3600 - val_loss: 1.7329 - val_accuracy: 0.3489\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.6060 - accuracy: 0.4083 - val_loss: 1.7307 - val_accuracy: 0.3239\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 1.5831 - accuracy: 0.3897 - val_loss: 1.7047 - val_accuracy: 0.3539\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 3s 72ms/step - loss: 1.5237 - accuracy: 0.4348 - val_loss: 1.6478 - val_accuracy: 0.4257\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 3s 73ms/step - loss: 1.5157 - accuracy: 0.4280 - val_loss: 1.6978 - val_accuracy: 0.3589\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 1.4570 - accuracy: 0.4592 - val_loss: 1.8743 - val_accuracy: 0.2688\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.4485 - accuracy: 0.4468 - val_loss: 1.5961 - val_accuracy: 0.4040\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 1.3739 - accuracy: 0.4783 - val_loss: 1.5643 - val_accuracy: 0.4174\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.3789 - accuracy: 0.4862 - val_loss: 1.5605 - val_accuracy: 0.4124\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.3000 - accuracy: 0.5216 - val_loss: 1.6038 - val_accuracy: 0.3923\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 1.3446 - accuracy: 0.4852 - val_loss: 1.5999 - val_accuracy: 0.4007\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.3116 - accuracy: 0.4937 - val_loss: 1.5698 - val_accuracy: 0.4124\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.3050 - accuracy: 0.5124 - val_loss: 1.5610 - val_accuracy: 0.4057\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.3108 - accuracy: 0.5021 - val_loss: 1.5258 - val_accuracy: 0.4157\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.2698 - accuracy: 0.5251 - val_loss: 1.6144 - val_accuracy: 0.3740\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 1.2959 - accuracy: 0.5090 - val_loss: 1.5690 - val_accuracy: 0.4124\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.2283 - accuracy: 0.5405 - val_loss: 1.5737 - val_accuracy: 0.4007\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 1.2728 - accuracy: 0.5053 - val_loss: 1.5554 - val_accuracy: 0.4324\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.2516 - accuracy: 0.5332 - val_loss: 1.5252 - val_accuracy: 0.4524\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.2695 - accuracy: 0.5162 - val_loss: 1.5820 - val_accuracy: 0.4023\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.2701 - accuracy: 0.5129 - val_loss: 1.5345 - val_accuracy: 0.4324\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 1.2343 - accuracy: 0.5322 - val_loss: 1.4800 - val_accuracy: 0.4624\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.2091 - accuracy: 0.5528 - val_loss: 1.5116 - val_accuracy: 0.4374\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.2069 - accuracy: 0.5415 - val_loss: 1.5227 - val_accuracy: 0.4391\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.1895 - accuracy: 0.5525 - val_loss: 1.6130 - val_accuracy: 0.4324\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 1.2418 - accuracy: 0.5218 - val_loss: 1.6269 - val_accuracy: 0.3856\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.2318 - accuracy: 0.5099 - val_loss: 1.5459 - val_accuracy: 0.4274\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.1909 - accuracy: 0.5348 - val_loss: 1.6333 - val_accuracy: 0.3706\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.2386 - accuracy: 0.5077 - val_loss: 1.5325 - val_accuracy: 0.4391\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.1901 - accuracy: 0.5317 - val_loss: 1.5499 - val_accuracy: 0.4307\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.2029 - accuracy: 0.5237 - val_loss: 1.4754 - val_accuracy: 0.4708\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.2230 - accuracy: 0.5242 - val_loss: 1.5920 - val_accuracy: 0.4057\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 1.1630 - accuracy: 0.5561 - val_loss: 1.5846 - val_accuracy: 0.4257\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.1603 - accuracy: 0.5513 - val_loss: 1.5546 - val_accuracy: 0.4157\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.1873 - accuracy: 0.5379 - val_loss: 1.5241 - val_accuracy: 0.4341\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.1933 - accuracy: 0.5187 - val_loss: 1.7968 - val_accuracy: 0.3623\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 1.1799 - accuracy: 0.5485 - val_loss: 1.5863 - val_accuracy: 0.4257\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.1854 - accuracy: 0.5343 - val_loss: 1.5519 - val_accuracy: 0.3973\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.1183 - accuracy: 0.5610 - val_loss: 1.7462 - val_accuracy: 0.3873\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.0994 - accuracy: 0.5875 - val_loss: 1.5715 - val_accuracy: 0.4374\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 1.1157 - accuracy: 0.5646 - val_loss: 1.4892 - val_accuracy: 0.4725\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.0687 - accuracy: 0.5776 - val_loss: 1.6204 - val_accuracy: 0.4107\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 1.0937 - accuracy: 0.5720 - val_loss: 1.5368 - val_accuracy: 0.4558\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.1037 - accuracy: 0.5837 - val_loss: 1.6144 - val_accuracy: 0.4207\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 1.0898 - accuracy: 0.5847 - val_loss: 1.5771 - val_accuracy: 0.4174\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.1200 - accuracy: 0.5573 - val_loss: 1.5473 - val_accuracy: 0.4441\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.1114 - accuracy: 0.5646 - val_loss: 1.6159 - val_accuracy: 0.4357\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.1359 - accuracy: 0.5674 - val_loss: 1.5728 - val_accuracy: 0.4441\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.0666 - accuracy: 0.5848 - val_loss: 1.5298 - val_accuracy: 0.4624\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.1079 - accuracy: 0.5738 - val_loss: 1.5845 - val_accuracy: 0.4274\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.1084 - accuracy: 0.5674 - val_loss: 1.5624 - val_accuracy: 0.4457\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 1.0676 - accuracy: 0.6002 - val_loss: 1.6164 - val_accuracy: 0.4374\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.0700 - accuracy: 0.5873 - val_loss: 1.5895 - val_accuracy: 0.4174\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 1.0664 - accuracy: 0.5878 - val_loss: 1.5385 - val_accuracy: 0.4491\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 1.0317 - accuracy: 0.5987 - val_loss: 1.5531 - val_accuracy: 0.4558\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.0435 - accuracy: 0.6005 - val_loss: 1.5786 - val_accuracy: 0.4591\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.0728 - accuracy: 0.5923 - val_loss: 1.5492 - val_accuracy: 0.4741\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.0402 - accuracy: 0.6081 - val_loss: 1.5773 - val_accuracy: 0.4725\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.0340 - accuracy: 0.6010 - val_loss: 1.5550 - val_accuracy: 0.4708\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 1.0073 - accuracy: 0.6206 - val_loss: 1.6666 - val_accuracy: 0.4324\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 3s 72ms/step - loss: 1.0947 - accuracy: 0.5751 - val_loss: 1.6210 - val_accuracy: 0.4508\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.0401 - accuracy: 0.5993 - val_loss: 1.6690 - val_accuracy: 0.4224\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.9954 - accuracy: 0.6247 - val_loss: 1.5948 - val_accuracy: 0.4641\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.0316 - accuracy: 0.6143 - val_loss: 1.7012 - val_accuracy: 0.4290\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 1.0264 - accuracy: 0.6066 - val_loss: 1.5719 - val_accuracy: 0.4841\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 0.9622 - accuracy: 0.6278 - val_loss: 1.6155 - val_accuracy: 0.4541\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 3s 76ms/step - loss: 1.0201 - accuracy: 0.5981 - val_loss: 1.6738 - val_accuracy: 0.4424\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 3s 75ms/step - loss: 1.0032 - accuracy: 0.6193 - val_loss: 1.8014 - val_accuracy: 0.4023\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 3s 73ms/step - loss: 0.9878 - accuracy: 0.6227 - val_loss: 1.6360 - val_accuracy: 0.4441\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 0.9838 - accuracy: 0.6125 - val_loss: 1.5884 - val_accuracy: 0.4624\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.9484 - accuracy: 0.6412 - val_loss: 1.6080 - val_accuracy: 0.4658\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 0.9199 - accuracy: 0.6559 - val_loss: 1.6294 - val_accuracy: 0.4658\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.9419 - accuracy: 0.6366 - val_loss: 1.6129 - val_accuracy: 0.4591\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.9375 - accuracy: 0.6483 - val_loss: 1.6102 - val_accuracy: 0.4374\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.9365 - accuracy: 0.6553 - val_loss: 1.5430 - val_accuracy: 0.4407\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.9786 - accuracy: 0.6170 - val_loss: 1.6992 - val_accuracy: 0.4508\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.9421 - accuracy: 0.6332 - val_loss: 1.9648 - val_accuracy: 0.4157\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.9419 - accuracy: 0.6336 - val_loss: 1.7966 - val_accuracy: 0.4457\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.8816 - accuracy: 0.6663 - val_loss: 1.6369 - val_accuracy: 0.4925\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.9098 - accuracy: 0.6469 - val_loss: 1.6274 - val_accuracy: 0.4858\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.9133 - accuracy: 0.6603 - val_loss: 1.7544 - val_accuracy: 0.4558\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 0.9463 - accuracy: 0.6404 - val_loss: 1.6295 - val_accuracy: 0.4608\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.9428 - accuracy: 0.6389 - val_loss: 1.6032 - val_accuracy: 0.4891\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.8925 - accuracy: 0.6606 - val_loss: 1.6282 - val_accuracy: 0.4474\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.9168 - accuracy: 0.6518 - val_loss: 1.6305 - val_accuracy: 0.4708\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.8885 - accuracy: 0.6701 - val_loss: 1.6158 - val_accuracy: 0.4741\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.9366 - accuracy: 0.6338 - val_loss: 1.6449 - val_accuracy: 0.4674\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 0.8851 - accuracy: 0.6669 - val_loss: 1.6889 - val_accuracy: 0.4541\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.8972 - accuracy: 0.6500 - val_loss: 1.7587 - val_accuracy: 0.4524\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 0.9017 - accuracy: 0.6557 - val_loss: 1.6479 - val_accuracy: 0.4658\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.8873 - accuracy: 0.6668 - val_loss: 1.9975 - val_accuracy: 0.4508\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 0.9075 - accuracy: 0.6543 - val_loss: 1.7385 - val_accuracy: 0.4691\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.8489 - accuracy: 0.6807 - val_loss: 1.8272 - val_accuracy: 0.4658\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 0.8708 - accuracy: 0.6736 - val_loss: 1.6761 - val_accuracy: 0.4591\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.9285 - accuracy: 0.6443 - val_loss: 1.6849 - val_accuracy: 0.4457\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.8922 - accuracy: 0.6611 - val_loss: 1.7803 - val_accuracy: 0.4307\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.8701 - accuracy: 0.6722 - val_loss: 1.7697 - val_accuracy: 0.4491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6ed68a3fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLjPpSkGTD1c",
        "outputId": "dcd2cfd7-931d-4b39-dc9f-a9dfbf301e26"
      },
      "source": [
        "vgg_features_test = pretrained_model.predict(x_test)\r\n",
        "\r\n",
        "predictions = model2.predict(vgg_features_test, verbose=1)\r\n",
        "\r\n",
        "y_true, y_pred = [],[]\r\n",
        "classes = encoder.classes_\r\n",
        "for idx, prediction in enumerate(predictions): \r\n",
        "    y_true.append(classes[np.argmax(y_test[idx])])\r\n",
        "    y_pred.append(classes[np.argmax(prediction)])\r\n",
        "    \r\n",
        "print(classification_report(y_pred, y_true))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 0s 7ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AK       0.47      0.49      0.48        76\n",
            "         BCC       0.55      0.60      0.58       101\n",
            "         BKL       0.50      0.46      0.48       116\n",
            "          DF       0.12      0.80      0.21         5\n",
            "         MEL       0.62      0.42      0.50       134\n",
            "          NV       0.42      0.52      0.46        81\n",
            "         SCC       0.56      0.44      0.49        75\n",
            "        VASC       0.45      0.91      0.61        11\n",
            "\n",
            "    accuracy                           0.49       599\n",
            "   macro avg       0.46      0.58      0.48       599\n",
            "weighted avg       0.53      0.49      0.50       599\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}