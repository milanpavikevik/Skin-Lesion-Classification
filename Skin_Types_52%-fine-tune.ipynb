{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skin-Types-Ext-RAM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milanpavikevik/Skin-Lesion-Classification/blob/main/Skin_Types_52%25-fine-tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL2tzXOzHS9b",
        "outputId": "78365329-7f1e-49bd-eab1-56e111643577"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhTMJR9ZHTGF",
        "outputId": "ec5614af-41d4-4b5e-f4d5-7ce072cc0361"
      },
      "source": [
        "import warnings                        # To ignore any warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "%matplotlib inline\r\n",
        "%pylab inline\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import itertools\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# Audio\r\n",
        "import librosa\r\n",
        "import librosa.display\r\n",
        "\r\n",
        "# Scikit learn\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.utils import class_weight\r\n",
        "\r\n",
        "# Keras\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D, MaxPool2D\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wEiZ7VZQiVD"
      },
      "source": [
        "testiraj golemina na slika 768x768 , 512x512 , 384x384 , 256x256 , 192x192"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUMaUf0lHTJL",
        "outputId": "997b9685-e584-45c9-d3de-993fb976fad4"
      },
      "source": [
        "INPUT_DIR = '/content/drive/MyDrive/Skin-Lesion/**'\r\n",
        " \r\n",
        "dataset = []\r\n",
        "for filename in glob.iglob(INPUT_DIR):\r\n",
        "    kolkuPoKlasa=0\r\n",
        "    print(filename)\r\n",
        "    for f in glob.iglob(filename+'/**'):\r\n",
        "      #print(f)\r\n",
        "      kolkuPoKlasa+=1\r\n",
        "      if (kolkuPoKlasa > 1000):\r\n",
        "        kolkuPoKlasa=0\r\n",
        "        break\r\n",
        "        \r\n",
        "    \r\n",
        "      \r\n",
        "      image= cv2.imread( f, cv2.COLOR_BGR2RGB)\r\n",
        "      image=cv2.resize(image, (256, 256),interpolation = cv2.INTER_AREA)\r\n",
        "      image=np.array(image)\r\n",
        "      image = image.astype('float32')\r\n",
        "      image /= 255 \r\n",
        "      \r\n",
        "      class_name = filename.split('/')[-1]\r\n",
        "      \r\n",
        "      dataset.append([image,class_name])\r\n",
        "      \r\n",
        "      \r\n",
        "    \r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Skin-Lesion/BCC\n",
            "/content/drive/MyDrive/Skin-Lesion/DF\n",
            "/content/drive/MyDrive/Skin-Lesion/AK\n",
            "/content/drive/MyDrive/Skin-Lesion/BKL\n",
            "/content/drive/MyDrive/Skin-Lesion/NV\n",
            "/content/drive/MyDrive/Skin-Lesion/VASC\n",
            "/content/drive/MyDrive/Skin-Lesion/MEL\n",
            "/content/drive/MyDrive/Skin-Lesion/SCC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YStzGE87HTMy",
        "outputId": "70141c9c-65d8-4fe4-e4a4-20765b7118af"
      },
      "source": [
        "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\r\n",
        "x_train = []\r\n",
        "y_train = []\r\n",
        "for img,klasa in train:\r\n",
        "  x_train.append(img)\r\n",
        "  y_train.append(klasa)\r\n",
        "\r\n",
        "x_test = []\r\n",
        "y_test = []\r\n",
        "for img,klasa in test[:int(len(test)*0.5)]:\r\n",
        "  x_test.append(img)\r\n",
        "  y_test.append(klasa)\r\n",
        "\r\n",
        "\r\n",
        "x_val = []\r\n",
        "y_val = []\r\n",
        "for img,klasa in test[int(len(test)*0.5):]:\r\n",
        "  x_val.append(img)\r\n",
        "  y_val.append(klasa)\r\n",
        "\r\n",
        "print(len(x_train),len(y_train))\r\n",
        "print(len(y_test),len(y_test))\r\n",
        "print(len(x_val),len(y_val))\r\n",
        "\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(y_train)\r\n",
        "\r\n",
        "y_train = encoder.transform(y_train)\r\n",
        "\r\n",
        "y_test =  encoder.transform(y_test)\r\n",
        "y_val =   encoder.transform(y_val)\r\n",
        "\r\n",
        "\r\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\r\n",
        "\r\n",
        "\r\n",
        "class_weights = {i : class_weights[i] for i in range(8)}\r\n",
        "\r\n",
        "print(class_weight)\r\n",
        "\r\n",
        "x_test = np.asarray(x_test)\r\n",
        "x_val = np.asarray(x_val)\r\n",
        "x_train = np.asarray(x_train)\r\n",
        "\r\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)\r\n",
        "x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], x_val.shape[2], 3)\r\n",
        "\r\n",
        "y_train = to_categorical(y_train)\r\n",
        "y_test = to_categorical(y_test)\r\n",
        "y_val = to_categorical(y_val)\r\n",
        "\r\n",
        "print(\"X train:\", x_train.shape)\r\n",
        "print(\"Y train:\", y_train.shape)\r\n",
        "print(\"X test:\", x_test.shape)\r\n",
        "print(\"Y test:\", y_test.shape)\r\n",
        "\r\n",
        "print(\"X validation:\", x_val.shape)\r\n",
        "print(\"Y validation:\", y_val.shape)\r\n",
        "\r\n",
        "print(class_weights)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4789 4789\n",
            "599 599\n",
            "599 599\n",
            "<module 'sklearn.utils.class_weight' from '/usr/local/lib/python3.7/dist-packages/sklearn/utils/class_weight.py'>\n",
            "X train: (4789, 256, 256, 3)\n",
            "Y train: (4789, 8)\n",
            "X test: (599, 256, 256, 3)\n",
            "Y test: (599, 8)\n",
            "X validation: (599, 256, 256, 3)\n",
            "Y validation: (599, 8)\n",
            "{0: 0.8479107648725213, 1: 0.7596763959390863, 2: 0.7492177722152691, 3: 3.401278409090909, 4: 0.7363161131611317, 5: 0.7436335403726708, 6: 1.2093434343434344, 7: 2.8919082125603865}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "_SmbIOPmHTPC",
        "outputId": "22fd655e-9b67-4096-c93e-71159a0d6c72"
      },
      "source": [
        "import matplotlib\r\n",
        "matplotlib.rcParams['figure.figsize'] = (14, 5)\r\n",
        "matplotlib.rcParams['font.size'] = 12\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "num_classes = 8\r\n",
        "\r\n",
        "xlass=[]\r\n",
        "\r\n",
        "for row in dataset:\r\n",
        "  xlass.append(row[1])\r\n",
        "print(len(xlass))\r\n",
        "xlass = pd.DataFrame(xlass)\r\n",
        "\r\n",
        "class_counts = xlass.value_counts()\r\n",
        "cmap = plt.cm.get_cmap(plt.cm.Set3, 10)\r\n",
        "colors = [cmap(i) for i in range(num_classes)]\r\n",
        "\r\n",
        "plt.barh(range(num_classes)[::-1], class_counts, tick_label=xlass[0].unique(),\r\n",
        "         color=colors)\r\n",
        "plt.title('Class distribution of dataset')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "xlass=[]\r\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0Rm11BCtW_Q",
        "outputId": "91984eee-ff46-4d68-93c2-bdeb027ff7c4"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]),\r\n",
        "                 activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=2))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "\r\n",
        "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=2))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "\r\n",
        "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=2))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "\r\n",
        "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=2))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(GlobalAveragePooling2D())\r\n",
        "\r\n",
        "model.add(Dense(len(encoder.classes_), activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001)\r\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 255, 255, 16)      208       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 127, 127, 16)      0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 127, 127, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 126, 126, 32)      2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 62, 62, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 31, 31, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 31, 31, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 30, 30, 128)       32896     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 15, 15, 128)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 8)                 1032      \n",
            "=================================================================\n",
            "Total params: 44,472\n",
            "Trainable params: 44,472\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4meu5npDtXK3",
        "outputId": "fd9cc792-1028-4000-931f-ca2b8336ac92"
      },
      "source": [
        "history = model.fit(x_train, y_train,\r\n",
        "                    batch_size=128,\r\n",
        "                    epochs=130,\r\n",
        "                    validation_data=(x_val, y_val),\r\n",
        "                    class_weight=class_weights)\r\n",
        "\r\n",
        "\r\n",
        "model_name = \"/content/skin-classical.h5\"\r\n",
        "model.save(model_name)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/130\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.9706 - accuracy: 0.2076 - val_loss: 1.9536 - val_accuracy: 0.2154\n",
            "Epoch 2/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.9199 - accuracy: 0.2236 - val_loss: 1.9228 - val_accuracy: 0.2404\n",
            "Epoch 3/130\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.9167 - accuracy: 0.2193 - val_loss: 1.9168 - val_accuracy: 0.2304\n",
            "Epoch 4/130\n",
            "38/38 [==============================] - 129s 3s/step - loss: 1.8940 - accuracy: 0.2360 - val_loss: 1.8989 - val_accuracy: 0.2104\n",
            "Epoch 5/130\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.8861 - accuracy: 0.2458 - val_loss: 1.8927 - val_accuracy: 0.2321\n",
            "Epoch 6/130\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.8631 - accuracy: 0.2506 - val_loss: 1.9088 - val_accuracy: 0.2053\n",
            "Epoch 7/130\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.8596 - accuracy: 0.2600 - val_loss: 1.8540 - val_accuracy: 0.2471\n",
            "Epoch 8/130\n",
            "38/38 [==============================] - 125s 3s/step - loss: 1.8289 - accuracy: 0.2708 - val_loss: 1.8440 - val_accuracy: 0.2521\n",
            "Epoch 9/130\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.8067 - accuracy: 0.2823 - val_loss: 1.8297 - val_accuracy: 0.2654\n",
            "Epoch 10/130\n",
            "38/38 [==============================] - 133s 4s/step - loss: 1.7975 - accuracy: 0.2796 - val_loss: 1.8243 - val_accuracy: 0.2671\n",
            "Epoch 11/130\n",
            "38/38 [==============================] - 127s 3s/step - loss: 1.7881 - accuracy: 0.2769 - val_loss: 1.8205 - val_accuracy: 0.2705\n",
            "Epoch 12/130\n",
            "38/38 [==============================] - 130s 3s/step - loss: 1.7687 - accuracy: 0.2959 - val_loss: 1.8029 - val_accuracy: 0.2738\n",
            "Epoch 13/130\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.7576 - accuracy: 0.2992 - val_loss: 1.7927 - val_accuracy: 0.2788\n",
            "Epoch 14/130\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.7494 - accuracy: 0.2953 - val_loss: 1.7777 - val_accuracy: 0.2838\n",
            "Epoch 15/130\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.7464 - accuracy: 0.2953 - val_loss: 1.7868 - val_accuracy: 0.2871\n",
            "Epoch 16/130\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.7462 - accuracy: 0.2900 - val_loss: 1.7884 - val_accuracy: 0.2821\n",
            "Epoch 17/130\n",
            "38/38 [==============================] - 129s 3s/step - loss: 1.7246 - accuracy: 0.3051 - val_loss: 1.7717 - val_accuracy: 0.2821\n",
            "Epoch 18/130\n",
            "38/38 [==============================] - 129s 3s/step - loss: 1.7215 - accuracy: 0.3028 - val_loss: 1.7738 - val_accuracy: 0.3072\n",
            "Epoch 19/130\n",
            "38/38 [==============================] - 136s 4s/step - loss: 1.7176 - accuracy: 0.3049 - val_loss: 1.7515 - val_accuracy: 0.3072\n",
            "Epoch 20/130\n",
            "38/38 [==============================] - 138s 4s/step - loss: 1.7063 - accuracy: 0.3241 - val_loss: 1.7753 - val_accuracy: 0.3055\n",
            "Epoch 21/130\n",
            "38/38 [==============================] - 133s 4s/step - loss: 1.7181 - accuracy: 0.3151 - val_loss: 1.7681 - val_accuracy: 0.3239\n",
            "Epoch 22/130\n",
            "38/38 [==============================] - 132s 3s/step - loss: 1.7046 - accuracy: 0.3199 - val_loss: 1.7715 - val_accuracy: 0.3055\n",
            "Epoch 23/130\n",
            "38/38 [==============================] - 130s 3s/step - loss: 1.6960 - accuracy: 0.3180 - val_loss: 1.7478 - val_accuracy: 0.2972\n",
            "Epoch 24/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6886 - accuracy: 0.3234 - val_loss: 1.7575 - val_accuracy: 0.3038\n",
            "Epoch 25/130\n",
            "38/38 [==============================] - 130s 3s/step - loss: 1.6976 - accuracy: 0.3220 - val_loss: 1.7870 - val_accuracy: 0.3055\n",
            "Epoch 26/130\n",
            "38/38 [==============================] - 137s 4s/step - loss: 1.7003 - accuracy: 0.3095 - val_loss: 1.7805 - val_accuracy: 0.3222\n",
            "Epoch 27/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6869 - accuracy: 0.3326 - val_loss: 1.7508 - val_accuracy: 0.3205\n",
            "Epoch 28/130\n",
            "38/38 [==============================] - 137s 4s/step - loss: 1.6861 - accuracy: 0.3201 - val_loss: 1.7726 - val_accuracy: 0.3222\n",
            "Epoch 29/130\n",
            "38/38 [==============================] - 130s 3s/step - loss: 1.6751 - accuracy: 0.3283 - val_loss: 1.7269 - val_accuracy: 0.3072\n",
            "Epoch 30/130\n",
            "38/38 [==============================] - 129s 3s/step - loss: 1.6861 - accuracy: 0.3224 - val_loss: 1.7629 - val_accuracy: 0.3088\n",
            "Epoch 31/130\n",
            "38/38 [==============================] - 129s 3s/step - loss: 1.6708 - accuracy: 0.3230 - val_loss: 1.7334 - val_accuracy: 0.3339\n",
            "Epoch 32/130\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.6915 - accuracy: 0.3268 - val_loss: 1.7530 - val_accuracy: 0.3389\n",
            "Epoch 33/130\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.6711 - accuracy: 0.3251 - val_loss: 1.7440 - val_accuracy: 0.3289\n",
            "Epoch 34/130\n",
            "38/38 [==============================] - 129s 3s/step - loss: 1.6771 - accuracy: 0.3234 - val_loss: 1.7450 - val_accuracy: 0.3172\n",
            "Epoch 35/130\n",
            "38/38 [==============================] - 129s 3s/step - loss: 1.6643 - accuracy: 0.3270 - val_loss: 1.7403 - val_accuracy: 0.3406\n",
            "Epoch 36/130\n",
            "38/38 [==============================] - 130s 3s/step - loss: 1.6779 - accuracy: 0.3214 - val_loss: 1.7829 - val_accuracy: 0.3088\n",
            "Epoch 37/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6614 - accuracy: 0.3166 - val_loss: 1.7485 - val_accuracy: 0.3389\n",
            "Epoch 38/130\n",
            "38/38 [==============================] - 132s 3s/step - loss: 1.6598 - accuracy: 0.3287 - val_loss: 1.7430 - val_accuracy: 0.3272\n",
            "Epoch 39/130\n",
            "38/38 [==============================] - 134s 4s/step - loss: 1.6579 - accuracy: 0.3226 - val_loss: 1.7200 - val_accuracy: 0.3306\n",
            "Epoch 40/130\n",
            "38/38 [==============================] - 133s 3s/step - loss: 1.6551 - accuracy: 0.3278 - val_loss: 1.7501 - val_accuracy: 0.3255\n",
            "Epoch 41/130\n",
            "38/38 [==============================] - 132s 3s/step - loss: 1.6696 - accuracy: 0.3339 - val_loss: 1.7450 - val_accuracy: 0.3289\n",
            "Epoch 42/130\n",
            "38/38 [==============================] - 143s 4s/step - loss: 1.6613 - accuracy: 0.3255 - val_loss: 1.7283 - val_accuracy: 0.3205\n",
            "Epoch 43/130\n",
            "38/38 [==============================] - 138s 4s/step - loss: 1.6412 - accuracy: 0.3333 - val_loss: 1.7329 - val_accuracy: 0.3272\n",
            "Epoch 44/130\n",
            "38/38 [==============================] - 132s 3s/step - loss: 1.6456 - accuracy: 0.3297 - val_loss: 1.7395 - val_accuracy: 0.3339\n",
            "Epoch 45/130\n",
            "38/38 [==============================] - 134s 4s/step - loss: 1.6587 - accuracy: 0.3333 - val_loss: 1.7578 - val_accuracy: 0.3272\n",
            "Epoch 46/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6538 - accuracy: 0.3268 - val_loss: 1.7422 - val_accuracy: 0.3222\n",
            "Epoch 47/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6522 - accuracy: 0.3234 - val_loss: 1.7256 - val_accuracy: 0.3222\n",
            "Epoch 48/130\n",
            "38/38 [==============================] - 135s 4s/step - loss: 1.6494 - accuracy: 0.3326 - val_loss: 1.7469 - val_accuracy: 0.3289\n",
            "Epoch 49/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6413 - accuracy: 0.3406 - val_loss: 1.7334 - val_accuracy: 0.3239\n",
            "Epoch 50/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6442 - accuracy: 0.3402 - val_loss: 1.7288 - val_accuracy: 0.3255\n",
            "Epoch 51/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6399 - accuracy: 0.3285 - val_loss: 1.7444 - val_accuracy: 0.3289\n",
            "Epoch 52/130\n",
            "38/38 [==============================] - 132s 3s/step - loss: 1.6420 - accuracy: 0.3289 - val_loss: 1.7364 - val_accuracy: 0.3172\n",
            "Epoch 53/130\n",
            "38/38 [==============================] - 133s 4s/step - loss: 1.6363 - accuracy: 0.3393 - val_loss: 1.7227 - val_accuracy: 0.3406\n",
            "Epoch 54/130\n",
            "38/38 [==============================] - 135s 4s/step - loss: 1.6425 - accuracy: 0.3316 - val_loss: 1.7262 - val_accuracy: 0.3339\n",
            "Epoch 55/130\n",
            "38/38 [==============================] - 133s 4s/step - loss: 1.6255 - accuracy: 0.3316 - val_loss: 1.7457 - val_accuracy: 0.3139\n",
            "Epoch 56/130\n",
            "38/38 [==============================] - 130s 3s/step - loss: 1.6511 - accuracy: 0.3264 - val_loss: 1.7434 - val_accuracy: 0.3456\n",
            "Epoch 57/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6339 - accuracy: 0.3333 - val_loss: 1.7292 - val_accuracy: 0.3406\n",
            "Epoch 58/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6325 - accuracy: 0.3253 - val_loss: 1.7475 - val_accuracy: 0.3222\n",
            "Epoch 59/130\n",
            "38/38 [==============================] - 134s 4s/step - loss: 1.6301 - accuracy: 0.3372 - val_loss: 1.7345 - val_accuracy: 0.3155\n",
            "Epoch 60/130\n",
            "38/38 [==============================] - 134s 4s/step - loss: 1.6297 - accuracy: 0.3351 - val_loss: 1.7176 - val_accuracy: 0.3289\n",
            "Epoch 61/130\n",
            "38/38 [==============================] - 133s 4s/step - loss: 1.6226 - accuracy: 0.3360 - val_loss: 1.7248 - val_accuracy: 0.3422\n",
            "Epoch 62/130\n",
            "38/38 [==============================] - 136s 4s/step - loss: 1.6284 - accuracy: 0.3383 - val_loss: 1.7439 - val_accuracy: 0.3205\n",
            "Epoch 63/130\n",
            "38/38 [==============================] - 132s 3s/step - loss: 1.6238 - accuracy: 0.3333 - val_loss: 1.7243 - val_accuracy: 0.3356\n",
            "Epoch 64/130\n",
            "38/38 [==============================] - 133s 4s/step - loss: 1.6149 - accuracy: 0.3414 - val_loss: 1.7246 - val_accuracy: 0.3272\n",
            "Epoch 65/130\n",
            "38/38 [==============================] - 132s 3s/step - loss: 1.6197 - accuracy: 0.3425 - val_loss: 1.7418 - val_accuracy: 0.3222\n",
            "Epoch 66/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6278 - accuracy: 0.3485 - val_loss: 1.7339 - val_accuracy: 0.3356\n",
            "Epoch 67/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6149 - accuracy: 0.3354 - val_loss: 1.7316 - val_accuracy: 0.3406\n",
            "Epoch 68/130\n",
            "38/38 [==============================] - 131s 3s/step - loss: 1.6183 - accuracy: 0.3418 - val_loss: 1.7445 - val_accuracy: 0.3306\n",
            "Epoch 69/130\n",
            "38/38 [==============================] - 132s 3s/step - loss: 1.6089 - accuracy: 0.3429 - val_loss: 1.7337 - val_accuracy: 0.3155\n",
            "Epoch 70/130\n",
            "38/38 [==============================] - 130s 3s/step - loss: 1.6226 - accuracy: 0.3374 - val_loss: 1.7197 - val_accuracy: 0.3356\n",
            "Epoch 71/130\n",
            "38/38 [==============================] - 130s 3s/step - loss: 1.6155 - accuracy: 0.3512 - val_loss: 1.7518 - val_accuracy: 0.3122\n",
            "Epoch 72/130\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.6052 - accuracy: 0.3410 - val_loss: 1.7669 - val_accuracy: 0.3105\n",
            "Epoch 73/130\n",
            "38/38 [==============================] - 128s 3s/step - loss: 1.6232 - accuracy: 0.3548 - val_loss: 1.7540 - val_accuracy: 0.3406\n",
            "Epoch 74/130\n",
            "38/38 [==============================] - 126s 3s/step - loss: 1.6088 - accuracy: 0.3414 - val_loss: 1.7412 - val_accuracy: 0.3372\n",
            "Epoch 75/130\n",
            "22/38 [================>.............] - ETA: 52s - loss: 1.5554 - accuracy: 0.3590"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDFPvny8t_oG"
      },
      "source": [
        "predictions = model.predict(x_test, verbose=1)\r\n",
        "\r\n",
        "y_true, y_pred = [],[]\r\n",
        "classes = encoder.classes_\r\n",
        "for idx, prediction in enumerate(predictions): \r\n",
        "    y_true.append(classes[np.argmax(y_test[idx])])\r\n",
        "    y_pred.append(classes[np.argmax(prediction)])\r\n",
        "    \r\n",
        "print(classification_report(y_pred, y_true))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWEjDP6_5UHu"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "# convolutional layer\r\n",
        "model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3])))\r\n",
        "\r\n",
        "# convolutional layer\r\n",
        "model.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "\r\n",
        "model.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "\r\n",
        "# flatten output of conv\r\n",
        "model.add(Flatten())\r\n",
        "\r\n",
        "# hidden layer\r\n",
        "model.add(Dense(500, activation='relu'))\r\n",
        "model.add(Dropout(0.4))\r\n",
        "model.add(Dense(250, activation='relu'))\r\n",
        "model.add(Dropout(0.3))\r\n",
        "# output layer\r\n",
        "model.add(Dense(8, activation='softmax'))\r\n",
        "\r\n",
        "# compiling the sequential model\r\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\r\n",
        "\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsCtEALu5URe",
        "outputId": "5f5b1c08-6e41-4eda-9ede-89d6fcea431a"
      },
      "source": [
        "history1 = model.fit(x_train, y_train,\r\n",
        "                    batch_size=128,\r\n",
        "                    epochs=10,\r\n",
        "                    validation_data=(x_val, y_val))\r\n",
        "\r\n",
        "\r\n",
        "model_name = \"/content/skin-type-1.h5\"\r\n",
        "model.save(model_name)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 1976s 52s/step - loss: 6.9064 - accuracy: 0.1548 - val_loss: 2.0622 - val_accuracy: 0.1436\n",
            "38/38 [==============================] - 1976s 52s/step - loss: 6.9064 - accuracy: 0.1548 - val_loss: 2.0622 - val_accuracy: 0.1436\n",
            "Epoch 2/10\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 1896s 50s/step - loss: 1.9537 - accuracy: 0.1754 - val_loss: 2.0376 - val_accuracy: 0.1486\n",
            "38/38 [==============================] - 1896s 50s/step - loss: 1.9537 - accuracy: 0.1754 - val_loss: 2.0376 - val_accuracy: 0.1486\n",
            "Epoch 3/10\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 1911s 50s/step - loss: 1.9217 - accuracy: 0.2030 - val_loss: 2.0407 - val_accuracy: 0.1386\n",
            "38/38 [==============================] - 1911s 50s/step - loss: 1.9217 - accuracy: 0.2030 - val_loss: 2.0407 - val_accuracy: 0.1386\n",
            "Epoch 4/10\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 1891s 50s/step - loss: 1.9180 - accuracy: 0.2287 - val_loss: 2.0201 - val_accuracy: 0.1369\n",
            "38/38 [==============================] - 1891s 50s/step - loss: 1.9180 - accuracy: 0.2287 - val_loss: 2.0201 - val_accuracy: 0.1369\n",
            "Epoch 5/10\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 1881s 50s/step - loss: 1.8863 - accuracy: 0.2482 - val_loss: 1.9558 - val_accuracy: 0.2137\n",
            "38/38 [==============================] - 1881s 50s/step - loss: 1.8863 - accuracy: 0.2482 - val_loss: 1.9558 - val_accuracy: 0.2137\n",
            "Epoch 6/10\n",
            "Epoch 6/10\n",
            "38/38 [==============================] - 1893s 50s/step - loss: 1.8627 - accuracy: 0.2596 - val_loss: 1.9403 - val_accuracy: 0.1836\n",
            "38/38 [==============================] - 1893s 50s/step - loss: 1.8627 - accuracy: 0.2596 - val_loss: 1.9403 - val_accuracy: 0.1836\n",
            "Epoch 7/10\n",
            "Epoch 7/10\n",
            "38/38 [==============================] - 1874s 49s/step - loss: 1.8316 - accuracy: 0.2731 - val_loss: 1.9476 - val_accuracy: 0.1970\n",
            "38/38 [==============================] - 1874s 49s/step - loss: 1.8316 - accuracy: 0.2731 - val_loss: 1.9476 - val_accuracy: 0.1970\n",
            "Epoch 8/10\n",
            "Epoch 8/10\n",
            "38/38 [==============================] - 1876s 49s/step - loss: 1.8099 - accuracy: 0.2673 - val_loss: 1.8651 - val_accuracy: 0.2571\n",
            "38/38 [==============================] - 1876s 49s/step - loss: 1.8099 - accuracy: 0.2673 - val_loss: 1.8651 - val_accuracy: 0.2571\n",
            "Epoch 9/10\n",
            "Epoch 9/10\n",
            "38/38 [==============================] - 1849s 49s/step - loss: 1.7562 - accuracy: 0.3059 - val_loss: 1.8237 - val_accuracy: 0.2671\n",
            "38/38 [==============================] - 1849s 49s/step - loss: 1.7562 - accuracy: 0.3059 - val_loss: 1.8237 - val_accuracy: 0.2671\n",
            "Epoch 10/10\n",
            "Epoch 10/10\n",
            "38/38 [==============================] - 1921s 51s/step - loss: 1.7090 - accuracy: 0.3240 - val_loss: 1.7423 - val_accuracy: 0.3005\n",
            "38/38 [==============================] - 1921s 51s/step - loss: 1.7090 - accuracy: 0.3240 - val_loss: 1.7423 - val_accuracy: 0.3005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lda0p1H-qRCl",
        "outputId": "00e96c76-5df5-4e88-eaab-8b015410011b"
      },
      "source": [
        "predictions = model.predict(x_test, verbose=1)\r\n",
        "\r\n",
        "y_true, y_pred = [],[]\r\n",
        "classes = encoder.classes_\r\n",
        "for idx, prediction in enumerate(predictions): \r\n",
        "    y_true.append(classes[np.argmax(y_test[idx])])\r\n",
        "    y_pred.append(classes[np.argmax(prediction)])\r\n",
        "    \r\n",
        "print(classification_report(y_pred, y_true))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 54s 3s/step\n",
            "19/19 [==============================] - 54s 3s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AK       0.37      0.30      0.33        96\n",
            "         BCC       0.52      0.34      0.41       171\n",
            "         BKL       0.22      0.25      0.23        91\n",
            "          DF       0.00      0.00      0.00         0\n",
            "         MEL       0.18      0.57      0.27        28\n",
            "          NV       0.49      0.40      0.44       124\n",
            "         SCC       0.36      0.28      0.32        74\n",
            "        VASC       0.32      0.47      0.38        15\n",
            "\n",
            "    accuracy                           0.34       599\n",
            "   macro avg       0.31      0.33      0.30       599\n",
            "weighted avg       0.40      0.34      0.36       599\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AK       0.37      0.30      0.33        96\n",
            "         BCC       0.52      0.34      0.41       171\n",
            "         BKL       0.22      0.25      0.23        91\n",
            "          DF       0.00      0.00      0.00         0\n",
            "         MEL       0.18      0.57      0.27        28\n",
            "          NV       0.49      0.40      0.44       124\n",
            "         SCC       0.36      0.28      0.32        74\n",
            "        VASC       0.32      0.47      0.38        15\n",
            "\n",
            "    accuracy                           0.34       599\n",
            "   macro avg       0.31      0.33      0.30       599\n",
            "weighted avg       0.40      0.34      0.36       599\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxh5qfQv6LIh"
      },
      "source": [
        "hyper-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPb3EoGp5UUE",
        "outputId": "05bee4d8-18e0-4f1f-9a94-988f2c8f1eb6"
      },
      "source": [
        "from keras.applications import VGG16\r\n",
        "\r\n",
        "# include top should be False to remove the softmax layer\r\n",
        "pretrained_model = VGG16(include_top=False, weights='imagenet')\r\n",
        "pretrained_model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "byX90SPN5UXo",
        "outputId": "f60f6c20-d3e6-4cd1-fe3c-79877c23341c"
      },
      "source": [
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "vgg_features_train = pretrained_model.predict(x_train)\r\n",
        "vgg_features_val = pretrained_model.predict(x_val)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b851c8875d9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvgg_features_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvgg_features_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:207 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer vgg16 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 256, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 8) dtype=float32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkIWRQtp6Tuf"
      },
      "source": [
        "train_target = y_train\r\n",
        "val_target = y_val"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Buz-AG6TyG",
        "outputId": "61a0721e-1cde-41f7-97e7-738c31366ca9"
      },
      "source": [
        "from keras.layers.normalization import BatchNormalization\r\n",
        "model2 = Sequential()\r\n",
        "model2.add(Flatten(input_shape=(8,8,512)))\r\n",
        "model2.add(Dense(100, activation='relu'))\r\n",
        "model2.add(Dropout(0.5))\r\n",
        "model2.add(BatchNormalization())\r\n",
        "model2.add(Dense(8, activation='softmax'))\r\n",
        "\r\n",
        "# compile the model\r\n",
        "model2.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\r\n",
        "\r\n",
        "model2.summary()\r\n",
        "\r\n",
        "# train model using features generated from VGG16 model\r\n",
        "model2.fit(vgg_features_train, train_target, epochs=100, batch_size=128, validation_data=(vgg_features_val, val_target),class_weight=class_weights)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_7 (Flatten)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               3276900   \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 8)                 808       \n",
            "=================================================================\n",
            "Total params: 3,278,108\n",
            "Trainable params: 3,277,908\n",
            "Non-trainable params: 200\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 2.2400 - accuracy: 0.1770 - val_loss: 2.0409 - val_accuracy: 0.2671\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.9336 - accuracy: 0.2188 - val_loss: 1.8242 - val_accuracy: 0.3205\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.8022 - accuracy: 0.2749 - val_loss: 1.8003 - val_accuracy: 0.2988\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.7230 - accuracy: 0.2982 - val_loss: 1.8609 - val_accuracy: 0.2955\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 1.6234 - accuracy: 0.3576 - val_loss: 1.8084 - val_accuracy: 0.2671\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.5464 - accuracy: 0.3637 - val_loss: 1.7684 - val_accuracy: 0.3255\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 1.5031 - accuracy: 0.4065 - val_loss: 1.7354 - val_accuracy: 0.3439\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 1.4997 - accuracy: 0.4212 - val_loss: 1.6797 - val_accuracy: 0.4124\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.4429 - accuracy: 0.4367 - val_loss: 1.6703 - val_accuracy: 0.3823\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.4066 - accuracy: 0.4392 - val_loss: 1.6100 - val_accuracy: 0.3923\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.3996 - accuracy: 0.4195 - val_loss: 1.6695 - val_accuracy: 0.3689\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.3832 - accuracy: 0.4413 - val_loss: 1.5468 - val_accuracy: 0.4107\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.3520 - accuracy: 0.4398 - val_loss: 1.5318 - val_accuracy: 0.4307\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.2910 - accuracy: 0.4695 - val_loss: 1.6145 - val_accuracy: 0.3740\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.3046 - accuracy: 0.4380 - val_loss: 1.9748 - val_accuracy: 0.2504\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.3651 - accuracy: 0.4380 - val_loss: 1.6000 - val_accuracy: 0.3840\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.3177 - accuracy: 0.4485 - val_loss: 1.5104 - val_accuracy: 0.4257\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 1.2625 - accuracy: 0.4654 - val_loss: 1.4709 - val_accuracy: 0.4574\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.2583 - accuracy: 0.4656 - val_loss: 1.6620 - val_accuracy: 0.3706\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.2307 - accuracy: 0.4915 - val_loss: 1.7980 - val_accuracy: 0.3172\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.2510 - accuracy: 0.4870 - val_loss: 1.5696 - val_accuracy: 0.3973\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.2231 - accuracy: 0.4982 - val_loss: 1.5117 - val_accuracy: 0.4057\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.2151 - accuracy: 0.4932 - val_loss: 1.4848 - val_accuracy: 0.4574\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.1326 - accuracy: 0.5164 - val_loss: 1.6193 - val_accuracy: 0.4040\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.1870 - accuracy: 0.4920 - val_loss: 1.5093 - val_accuracy: 0.4524\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.2084 - accuracy: 0.4935 - val_loss: 1.4880 - val_accuracy: 0.4391\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.1412 - accuracy: 0.5143 - val_loss: 1.4528 - val_accuracy: 0.4708\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.0944 - accuracy: 0.5314 - val_loss: 1.5878 - val_accuracy: 0.4090\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.1424 - accuracy: 0.5262 - val_loss: 1.4847 - val_accuracy: 0.4290\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 1.0716 - accuracy: 0.5543 - val_loss: 1.4555 - val_accuracy: 0.4725\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.0679 - accuracy: 0.5488 - val_loss: 1.4119 - val_accuracy: 0.4608\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.0369 - accuracy: 0.5762 - val_loss: 1.4708 - val_accuracy: 0.4474\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.0719 - accuracy: 0.5422 - val_loss: 1.5406 - val_accuracy: 0.4290\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.0663 - accuracy: 0.5575 - val_loss: 1.5528 - val_accuracy: 0.4274\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.0428 - accuracy: 0.5706 - val_loss: 2.2437 - val_accuracy: 0.2821\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.1324 - accuracy: 0.5316 - val_loss: 1.5268 - val_accuracy: 0.4608\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.1381 - accuracy: 0.5310 - val_loss: 1.6477 - val_accuracy: 0.4174\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.0636 - accuracy: 0.5612 - val_loss: 1.4446 - val_accuracy: 0.4741\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 1.0223 - accuracy: 0.5767 - val_loss: 1.5275 - val_accuracy: 0.4407\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 1.0194 - accuracy: 0.5712 - val_loss: 1.4472 - val_accuracy: 0.4908\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.9999 - accuracy: 0.5881 - val_loss: 1.5332 - val_accuracy: 0.4508\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.9750 - accuracy: 0.6099 - val_loss: 1.4015 - val_accuracy: 0.4858\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.9740 - accuracy: 0.6005 - val_loss: 1.5652 - val_accuracy: 0.4474\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.9860 - accuracy: 0.5838 - val_loss: 1.5263 - val_accuracy: 0.4641\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.1256 - accuracy: 0.5308 - val_loss: 1.9537 - val_accuracy: 0.3756\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 1.1146 - accuracy: 0.5422 - val_loss: 1.5643 - val_accuracy: 0.4558\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.0499 - accuracy: 0.5603 - val_loss: 1.5960 - val_accuracy: 0.4090\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 1.1196 - accuracy: 0.5416 - val_loss: 1.5296 - val_accuracy: 0.4508\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 1.0715 - accuracy: 0.5565 - val_loss: 1.5727 - val_accuracy: 0.4791\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.0016 - accuracy: 0.5723 - val_loss: 1.6065 - val_accuracy: 0.4441\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.0430 - accuracy: 0.5532 - val_loss: 1.5450 - val_accuracy: 0.4240\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 1.1274 - accuracy: 0.5475 - val_loss: 1.4931 - val_accuracy: 0.4474\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 1.0915 - accuracy: 0.5321 - val_loss: 1.4710 - val_accuracy: 0.4891\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 1.0017 - accuracy: 0.5761 - val_loss: 1.4807 - val_accuracy: 0.4908\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 1.0026 - accuracy: 0.5772 - val_loss: 1.4600 - val_accuracy: 0.4958\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.9852 - accuracy: 0.5821 - val_loss: 1.4470 - val_accuracy: 0.4908\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.9702 - accuracy: 0.6013 - val_loss: 1.4751 - val_accuracy: 0.4841\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.9817 - accuracy: 0.5787 - val_loss: 1.4756 - val_accuracy: 0.4841\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.9250 - accuracy: 0.6066 - val_loss: 1.4290 - val_accuracy: 0.4908\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.9259 - accuracy: 0.6144 - val_loss: 1.4671 - val_accuracy: 0.5008\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.9166 - accuracy: 0.5960 - val_loss: 1.4517 - val_accuracy: 0.4825\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.9482 - accuracy: 0.5854 - val_loss: 1.4756 - val_accuracy: 0.4791\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.9054 - accuracy: 0.6150 - val_loss: 1.4525 - val_accuracy: 0.4925\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.9407 - accuracy: 0.6001 - val_loss: 1.5480 - val_accuracy: 0.4725\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.8923 - accuracy: 0.6205 - val_loss: 1.5322 - val_accuracy: 0.4791\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.8824 - accuracy: 0.6361 - val_loss: 1.4936 - val_accuracy: 0.4975\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.9103 - accuracy: 0.6266 - val_loss: 1.8238 - val_accuracy: 0.4357\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.9500 - accuracy: 0.5987 - val_loss: 1.5809 - val_accuracy: 0.4891\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.8959 - accuracy: 0.6267 - val_loss: 1.5459 - val_accuracy: 0.4741\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.9605 - accuracy: 0.6078 - val_loss: 1.9389 - val_accuracy: 0.3856\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.9550 - accuracy: 0.6014 - val_loss: 1.5074 - val_accuracy: 0.4574\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.9622 - accuracy: 0.5994 - val_loss: 1.5116 - val_accuracy: 0.4441\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.9393 - accuracy: 0.6021 - val_loss: 2.1736 - val_accuracy: 0.3539\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 1.0157 - accuracy: 0.5857 - val_loss: 1.4863 - val_accuracy: 0.4474\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.9385 - accuracy: 0.6170 - val_loss: 1.5711 - val_accuracy: 0.4508\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.9205 - accuracy: 0.6298 - val_loss: 1.5173 - val_accuracy: 0.4975\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.8621 - accuracy: 0.6535 - val_loss: 1.5945 - val_accuracy: 0.4357\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.8984 - accuracy: 0.6156 - val_loss: 1.5110 - val_accuracy: 0.4674\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.9107 - accuracy: 0.6277 - val_loss: 1.4941 - val_accuracy: 0.4942\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.9229 - accuracy: 0.6120 - val_loss: 1.5219 - val_accuracy: 0.4775\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.8863 - accuracy: 0.6158 - val_loss: 1.5662 - val_accuracy: 0.4374\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.9174 - accuracy: 0.6055 - val_loss: 1.5912 - val_accuracy: 0.4307\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.9122 - accuracy: 0.6265 - val_loss: 1.5621 - val_accuracy: 0.4691\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.8965 - accuracy: 0.6192 - val_loss: 1.6174 - val_accuracy: 0.4591\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.9031 - accuracy: 0.6299 - val_loss: 1.6124 - val_accuracy: 0.4541\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.9523 - accuracy: 0.6103 - val_loss: 1.5207 - val_accuracy: 0.4875\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.9096 - accuracy: 0.6148 - val_loss: 1.5745 - val_accuracy: 0.4374\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 1.0026 - accuracy: 0.5828 - val_loss: 1.5398 - val_accuracy: 0.4641\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.9039 - accuracy: 0.6390 - val_loss: 1.5090 - val_accuracy: 0.4875\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.8852 - accuracy: 0.6365 - val_loss: 1.6078 - val_accuracy: 0.4875\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.8648 - accuracy: 0.6373 - val_loss: 1.6146 - val_accuracy: 0.4324\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.8595 - accuracy: 0.6365 - val_loss: 1.5499 - val_accuracy: 0.4708\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.9375 - accuracy: 0.6123 - val_loss: 1.6278 - val_accuracy: 0.4775\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 1.0075 - accuracy: 0.6026 - val_loss: 1.6455 - val_accuracy: 0.4441\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 3s 66ms/step - loss: 0.9772 - accuracy: 0.6231 - val_loss: 1.6214 - val_accuracy: 0.4257\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 1.0380 - accuracy: 0.5773 - val_loss: 1.5930 - val_accuracy: 0.4791\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.9375 - accuracy: 0.6292 - val_loss: 1.6679 - val_accuracy: 0.4758\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.9446 - accuracy: 0.6149 - val_loss: 1.5517 - val_accuracy: 0.4624\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.9428 - accuracy: 0.6047 - val_loss: 1.6480 - val_accuracy: 0.4808\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.9115 - accuracy: 0.6217 - val_loss: 1.5826 - val_accuracy: 0.4992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6edc099550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLjPpSkGTD1c",
        "outputId": "ea39c9af-93d9-412f-e5b0-52a403e6418a"
      },
      "source": [
        "vgg_features_test = pretrained_model.predict(x_test)\r\n",
        "\r\n",
        "predictions = model2.predict(vgg_features_test, verbose=1)\r\n",
        "\r\n",
        "y_true, y_pred = [],[]\r\n",
        "classes = encoder.classes_\r\n",
        "for idx, prediction in enumerate(predictions): \r\n",
        "    y_true.append(classes[np.argmax(y_test[idx])])\r\n",
        "    y_pred.append(classes[np.argmax(prediction)])\r\n",
        "    \r\n",
        "print(classification_report(y_pred, y_true))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 0s 7ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AK       0.51      0.47      0.48        86\n",
            "         BCC       0.58      0.56      0.57       115\n",
            "         BKL       0.52      0.47      0.50       117\n",
            "          DF       0.33      0.44      0.38        25\n",
            "         MEL       0.51      0.52      0.51        89\n",
            "          NV       0.49      0.53      0.51        92\n",
            "         SCC       0.64      0.58      0.61        65\n",
            "        VASC       0.41      0.90      0.56        10\n",
            "\n",
            "    accuracy                           0.52       599\n",
            "   macro avg       0.50      0.56      0.52       599\n",
            "weighted avg       0.53      0.52      0.52       599\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}